---
title: "Evaluation-Lab"
author: "Aishwarya Gavili, Anna Stein, Tatev Gomtsyan"
date: "10/27/2021"
output: html_document
---

Need to use 2 datasets. 


```{r}
# load in needed packages
install.packages("class")  
library(tidyverse)
library(caret)
library(RColorBrewer)
library(ROCR)
#install.packages("MLmetrics")
library(MLmetrics)
library(class)
```

```{r}
# read in data
music_data = read.csv("/Users/aishgav/Desktop/ds3001/DS3001/Evaluation-Lab/data.csv")
```


```{r}
# taking out NAs
table(is.na(music_data)) # from this, it seems that there aren't NAs in this dataframe 
music_data <- music_data[complete.cases(music_data), ] # take out rows that contain NAs
View(music_data) # looks like that didn't make a difference


# taking out unecessary columns: column titled 'X', column titled 'target'
music_data = music_data[,-c(1,15)] # don't run this more than once 
View(music_data)


# check variable classes, and change if necessary: 
str(music_data)
# classes seem to be correct, don't need changing

class(music_data$time_signature)
music_data$time_signature = as.character(music_data$time_signature)
class(music_data$time_signature)


numvar_inmusic = names(select_if(music_data, is.numeric))
#View(numvar_inmusic)
# this gives us a table with the columns that are numeric

# scale the numeric variables
music_data[numvar_inmusic] = lapply(music_data[numvar_inmusic], scale)


# normalize numeric variables 
# building a normalizer: 
normalize <- function(x){
 (x - min(x)) / (max(x) - min(x))
}



music_data[numvar_inmusic] = lapply(music_data[numvar_inmusic], normalize)
View(music_data)
# now, all numeric variables are normalized 
```

Need to make danceability into a binary variable? 
```{r}
# need to cut at a certain value for danceability
fivenum(music_data$danceability)
# the median is around 0.6 for danceability, so lets cut there. if danceability is above a 0.6, the song is highly danceable. Otherwise, the song has a low level of danceability. 
music_data$danceability <- cut(music_data$danceability,c(-1,0.6,1),labels = c(0,1))
View(music_data)
# now we have danceability as a binary variable 
```


Need to define a question that can be answered using kNN. 
we could have different categories of danceability -- low danceability, medium, and high. 
then, the kNN model would classify whether a song is low, medium, or high in terms of danceability. 
We could also use binary classification -- low or high danceability. 


```{r}


# This means that at random, we have an 11.6% chance of correctly picking
# out a subscribed individual. Let's see if kNN can do any better.



part_index_1 <- createDataPartition(music_data$danceability,
                                           times=1,
                                           p = 0.70,
                                           groups=1,
                                           list=FALSE)
View(part_index_1)

train <- music_data[part_index_1,]
tune_and_test <- music_data[-part_index_1, ]

#The we need to use the function again to create the tuning set 

tune_and_test_index <- createDataPartition(tune_and_test$danceability,
                                           p = .5,
                                           list = FALSE,
                                           times = 1)

tune <- tune_and_test[tune_and_test_index, ]
test <- tune_and_test[-tune_and_test_index, ]

dim(train)
dim(tune)
dim(test)



```





Training the classifer: 
```{r}
set.seed(200)
music_7NN<-  knn(train = train[, c("acousticness", "energy", "instrumentalness","liveness","tempo")],#<- training set cases # the variables we want to look at OTHER than danceability (i'm pretty sure)
               test = tune[, c("acousticness", "energy", "instrumentalness","liveness","tempo")],    #<- test set cases
               cl = train[, "danceability"],#<- category for true classification # this would be danceability????
               k = 7,#<- number of neighbors considered
               use.all = TRUE,
               prob = TRUE) #<- control ties between class assignments If true, all distances equal to the kth largest are included

```

Output of training classifier 
```{r}
str(music_7NN)
#table(music_7NN)
#length(music_7NN)

```

```{r}
kNN_res = table(music_7NN,
                tune$danceability)
#kNN_res
sum(kNN_res)

```


```{r}
#selecting true negatives and true positives 
kNN_res[row(kNN_res) == col(kNN_res)]

#accuracy
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc
```

```{r}
confusionMatrix(as.factor(music_7NN), as.factor(tune$danceability), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
```
Change k for higher accuracy

```{r}
# How does "k" affect classification accuracy? Let's create a function
# to calculate classification accuracy based on the number of "k."
chooseK = function(k, train_set, val_set, train_class, val_class){
  
  # Build knn with k neighbors considered.
  set.seed(200)
  class_knn = knn(train = train_set,    #<- training set cases
                  test = val_set,       #<- test set cases
                  cl = train_class,     #<- category for classification
                  k = k,                #<- number of neighbors considered
                  use.all = TRUE)       #<- control ties between class assignments
                                        #   If true, all distances equal to the kth 
                                        #   largest are included
  conf_mat = table(class_knn, val_class)
  
  # Calculate the accuracy.
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)                         
  cbind(k = k, accuracy = accu)
}

```



```{r}
# The sapply() function plugs in several values into our chooseK function.
#sapply(x, fun...) "fun" here is passing a function to our k-function
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop! Returns a matrix.
set.seed(200)
knn_different_k = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x, 
                          train_set = train[, c("acousticness", "energy", "instrumentalness","liveness","tempo")],
                          val_set = tune[, c("acousticness", "energy", "instrumentalness","liveness","tempo")],
                          train_class = train$danceability,
                          val_class = tune$danceability))



#A bit more of a explanation...
seq(1,21, by=2)#just creates a series of numbers
sapply(seq(1, 21, by=2), function(x) x+1)#sapply returns a new vector using the series of numbers and some calculation that is repeated over the vector of numbers 


# Reformating the results to graph
View(knn_different_k)
class(knn_different_k)#matrix 
head(knn_different_k)

knn_different_k = data.frame(k = knn_different_k[1,],
                             accuracy = knn_different_k[2,])

# Plot accuracy vs. k.

ggplot(knn_different_k,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)
```


Run with optimal k: k = 17

```{r}

set.seed(200)
music_17NN<-  knn(train = train[, c("acousticness", "energy", "instrumentalness","liveness","tempo")],#<- training set cases # the variables we want to look at OTHER than danceability (i'm pretty sure)
               test = tune[, c("acousticness", "energy", "instrumentalness","liveness","tempo")],    #<- test set cases
               cl = train[, "danceability"],#<- category for true classification # this would be danceability????
               k = 17,#<- number of neighbors considered
               use.all = TRUE,
               prob = TRUE) #<- control ties between class assignments If true, all distances equal to the kth largest are included

str(music_17NN)

```

```{r}
kNN_res_17 = table(music_17NN,
                tune$danceability)
#kNN_res
sum(kNN_res_17)

kNN_res_17[row(kNN_res_17) == col(kNN_res_17)]

#accuracy
kNN_acc_17 = sum(kNN_res_17[row(kNN_res_17) == col(kNN_res_17)]) / sum(kNN_res_17)
kNN_acc_17
```

```{r}
confusionMatrix(as.factor(music_17NN), as.factor(tune$danceability), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
```
Kappa = 0.3912 
Accuracy = 0.6931 

```{r}
tpr_17 <- 114/(114+30) #TP/TP+FN
fpr_17 <- 63/(63+96) #FP/FP+TN
```

TPR = .7917 or 79.17%
FPR = .3962 or 39.62%

```{r}
#F1 score computed using formula

f1_17NN <- (2 * tpr_17 * fpr_17)/(tpr_17 + fpr_17)
```

```{r}
#example code from brian - should be able to take out
#bank_3NN$pos_prec <- ifelse(bank_3NN$pred == 0, 1-bank_3NN$bank_prob, bank_3NN$bank_prob)
```


```{r}
#creating dataframe to help pass in parameters for metrics 
predicted_prob_17NN <- attributes(music_17NN)$prob #predicted probabilities for each class

pred_label_17NN <- head(music_17NN, n = 303) #predicted labels (0 or 1)

target_7NN <- tune$danceability #actual labels 
```

```{r}
df_17NN <- data.frame(pred_label_17NN, predicted_prob_17NN, target_7NN) 
df_17NN$pos_prec <- ifelse(df_17NN$pred_label_17NN == 0, 1-df_17NN$predicted_prob_17NN, df_17NN$predicted_prob_17NN) #predicted positive class probabilities
```

LogLoss
```{r}
LogLoss(as.numeric(df_17NN$ predicted_prob_17NN), as.numeric(df_17NN$target_7NN))
```

ROC 
```{r}
```

F1 Score
```{r}
#needs to be done using function
#F1_Score(y_pred = df_17NN$pred_label_17NN == 0, y_true = df_17NN$target_7NN, positive = "1")
```